{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f09a6b6",
   "metadata": {},
   "source": [
    "### To implement some functions of this activity I have used the help of LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979fdcc4",
   "metadata": {},
   "source": [
    "My personal github repo for this subject: https://github.com/carlos89p/Advanced-Machine-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdb732f",
   "metadata": {},
   "source": [
    "## Library import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16cd40ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d14205",
   "metadata": {},
   "source": [
    "## CNN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "855c6e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FashionCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Cálculo automático del tamaño de entrada a fc1\n",
    "        self._to_linear = None\n",
    "        self._get_flatten_size()\n",
    "        \n",
    "        self.fc1 = nn.Linear(self._to_linear, 256)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(256, 10)  # 10 clases en FashionMNIST\n",
    "\n",
    "    def _get_flatten_size(self):\n",
    "        \"\"\"Calcula el tamaño de entrada para fc1 ejecutando una pasada con datos de prueba.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            sample_input = torch.zeros(1, 1, 28, 28)  # Imagen de prueba 1x28x28\n",
    "            output = self.pool(F.relu(self.bn1(self.conv1(sample_input))))\n",
    "            output = self.pool(F.relu(self.bn2(self.conv2(output))))\n",
    "            output = self.pool(F.relu(self.bn3(self.conv3(output))))\n",
    "            output = self.pool(F.relu(self.bn4(self.conv4(output))))\n",
    "            self._to_linear = output.numel()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.pool(F.relu(self.bn4(self.conv4(x))))\n",
    "        \n",
    "        x = x.view(x.size(0), -1)  # Aplanar correctamente\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9977d75d",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ed8ae2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1000, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f7c8a5",
   "metadata": {},
   "source": [
    "## Model configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e1ce8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "model = FashionCNN().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629e0891",
   "metadata": {},
   "source": [
    "## Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d28912f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.3308822810649872\n",
      "Epoch 2, Loss: 0.17786630988121033\n",
      "Epoch 3, Loss: 0.17027172446250916\n",
      "Epoch 4, Loss: 0.036168817430734634\n",
      "Epoch 5, Loss: 0.31223368644714355\n",
      "Epoch 6, Loss: 0.08663536608219147\n",
      "Epoch 7, Loss: 0.15267667174339294\n",
      "Epoch 8, Loss: 0.13604311645030975\n",
      "Epoch 9, Loss: 0.019858337938785553\n",
      "Epoch 10, Loss: 0.022205373272299767\n",
      "Epoch 11, Loss: 0.1941538155078888\n",
      "Epoch 12, Loss: 0.041313111782073975\n",
      "Epoch 13, Loss: 0.04856882244348526\n",
      "Epoch 14, Loss: 0.002854670397937298\n",
      "Epoch 15, Loss: 0.105343297123909\n",
      "Epoch 16, Loss: 0.19301873445510864\n",
      "Epoch 17, Loss: 0.012712798081338406\n",
      "Epoch 18, Loss: 0.0567750558257103\n",
      "Epoch 19, Loss: 0.001072875689715147\n",
      "Epoch 20, Loss: 0.006679262034595013\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    for epoch in range(20):\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "\n",
    "train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fac1f1",
   "metadata": {},
   "source": [
    "## ONNX Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7532ce82",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 1, 28, 28, device=device)\n",
    "torch.onnx.export(model, x, \"fashion_mnist_cnn_prueba.onnx\", export_params=True, opset_version=11)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1f2329",
   "metadata": {},
   "source": [
    "## OpenCV implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d0dbb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-20 15:19:59.280 python[2643:4481887] WARNING: AVCaptureDeviceTypeExternal is deprecated for Continuity Cameras. Please use AVCaptureDeviceTypeContinuityCamera and add NSCameraUseContinuityCameraDeviceType to your Info.plist.\n",
      "2025-02-20 15:20:01.753 python[2643:4481887] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-02-20 15:20:01.753 python[2643:4481887] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mEl kernel se bloqueó al ejecutar código en la celda actual o en una celda anterior. \n",
      "\u001b[1;31mRevise el código de las celdas para identificar una posible causa del error. \n",
      "\u001b[1;31mHaga clic <a href='https://aka.ms/vscodeJupyterKernelCrash'>aquí</a> para obtener más información. \n",
      "\u001b[1;31mVea Jupyter <a href='command:jupyter.viewOutput'>log</a> para obtener más detalles."
     ]
    }
   ],
   "source": [
    "class_labels = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "session = ort.InferenceSession(\"fashion_mnist_cnn_prueba.onnx\")\n",
    "input_name = session.get_inputs()[0].name\n",
    "output_name = session.get_outputs()[0].name\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    resized = cv2.resize(gray, (28, 28))\n",
    "    normalized = resized.astype(np.float32) / 255.0\n",
    "    input_tensor = normalized.reshape(1, 1, 28, 28)\n",
    "    preds = session.run([output_name], {input_name: input_tensor})[0]\n",
    "    label = np.argmax(preds)\n",
    "    label_name = class_labels[label]\n",
    "    cv2.putText(frame, f'{label_name}', (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "    cv2.imshow('Fashion MNIST Real-time', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
