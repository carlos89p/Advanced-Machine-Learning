{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f09a6b6",
   "metadata": {},
   "source": [
    "### To implement some functions of this activity I have used the help of LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979fdcc4",
   "metadata": {},
   "source": [
    "My personal github repo for this subject: https://github.com/carlos89p/Advanced-Machine-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdb732f",
   "metadata": {},
   "source": [
    "## Library import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16cd40ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d14205",
   "metadata": {},
   "source": [
    "## CNN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "855c6e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FashionCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.fc1 = nn.Linear(1600, 128)  # Corrected input size\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.conv1(x))\n",
    "        x = nn.functional.max_pool2d(x, 2)\n",
    "        x = nn.functional.relu(self.conv2(x))\n",
    "        x = nn.functional.max_pool2d(x, 2)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9977d75d",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ed8ae2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1000, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f7c8a5",
   "metadata": {},
   "source": [
    "## Model configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e1ce8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = FashionCNN().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629e0891",
   "metadata": {},
   "source": [
    "## Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d28912f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.44927507638931274\n",
      "Epoch 2, Loss: 0.2754734456539154\n",
      "Epoch 3, Loss: 0.402096688747406\n",
      "Epoch 4, Loss: 0.1705428659915924\n",
      "Epoch 5, Loss: 0.1919562965631485\n",
      "Epoch 6, Loss: 0.13107222318649292\n",
      "Epoch 7, Loss: 0.20372720062732697\n",
      "Epoch 8, Loss: 0.30768758058547974\n",
      "Epoch 9, Loss: 0.026319611817598343\n",
      "Epoch 10, Loss: 0.25358307361602783\n",
      "Epoch 11, Loss: 0.043912723660469055\n",
      "Epoch 12, Loss: 0.08208343386650085\n",
      "Epoch 13, Loss: 0.061667729169130325\n",
      "Epoch 14, Loss: 0.020355459302663803\n",
      "Epoch 15, Loss: 0.1074550598859787\n",
      "Epoch 16, Loss: 0.048156049102544785\n",
      "Epoch 17, Loss: 0.019799571484327316\n",
      "Epoch 18, Loss: 0.031227780506014824\n",
      "Epoch 19, Loss: 0.03696593642234802\n",
      "Epoch 20, Loss: 0.035845234990119934\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    for epoch in range(20):\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "\n",
    "train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fac1f1",
   "metadata": {},
   "source": [
    "## ONNX Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7532ce82",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 1, 28, 28, device=device)\n",
    "torch.onnx.export(model, x, \"fashion_mnist_cnn.onnx\", export_params=True, opset_version=11)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1f2329",
   "metadata": {},
   "source": [
    "## OpenCV implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d0dbb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 13:26:45.052 python[97944:4197359] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-02-19 13:26:45.052 python[97944:4197359] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mEl kernel se bloqueó al ejecutar código en la celda actual o en una celda anterior. \n",
      "\u001b[1;31mRevise el código de las celdas para identificar una posible causa del error. \n",
      "\u001b[1;31mHaga clic <a href='https://aka.ms/vscodeJupyterKernelCrash'>aquí</a> para obtener más información. \n",
      "\u001b[1;31mVea Jupyter <a href='command:jupyter.viewOutput'>log</a> para obtener más detalles."
     ]
    }
   ],
   "source": [
    "class_labels = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "session = ort.InferenceSession(\"fashion_mnist_cnn.onnx\")\n",
    "input_name = session.get_inputs()[0].name\n",
    "output_name = session.get_outputs()[0].name\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    resized = cv2.resize(gray, (28, 28))\n",
    "    normalized = resized.astype(np.float32) / 255.0\n",
    "    input_tensor = normalized.reshape(1, 1, 28, 28)\n",
    "    preds = session.run([output_name], {input_name: input_tensor})[0]\n",
    "    label = np.argmax(preds)\n",
    "    label_name = class_labels[label]\n",
    "    cv2.putText(frame, f'{label_name}', (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "    cv2.imshow('Fashion MNIST Real-time', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
